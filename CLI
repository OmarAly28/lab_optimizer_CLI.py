import numpy as np
from skopt import Optimizer
from skopt.space import Real
from skopt.learning import GaussianProcessRegressor, RandomForestRegressor, ExtraTreesRegressor
import warnings

# Suppress scikit-optimize warnings
warnings.filterwarnings("ignore", category=UserWarning)

def get_valid_input(prompt, type_func, validation_func=None, error_message="Invalid input. Please try again."):
    """Helper function to get validated input from the user."""
    while True:
        try:
            value = type_func(input(prompt))
            if validation_func and not validation_func(value):
                print(error_message)
                continue
            return value
        except ValueError:
            print(error_message)

def format_param_value(value):
    """Format parameter value for display"""
    return f"{value:.4f}" if isinstance(value, float) else str(value)

def define_parameters():
    """Asks the user to define input parameters."""
    print("\n--- Define Your Reaction Parameters ---")
    num_params = get_valid_input(
        "How many input parameters will you be optimizing? (e.g., Temperature, Concentration): ",
        int,
        lambda x: x > 0,
        "Please enter a positive integer."
    )

    dimensions = []
    param_names = []

    for i in range(num_params):
        print(f"\nParameter {i + 1}:")
        name = get_valid_input(f"Enter name for parameter {i + 1}: ", str).strip()
        while name in param_names:
            print("Parameter name already exists. Please enter a unique name.")
            name = get_valid_input(f"Enter name for parameter {i + 1}: ", str).strip()

        param_names.append(name)
        print(f"Parameter '{name}' will be treated as continuous.")
        
        low = get_valid_input(f"Enter minimum value for '{name}': ", float)
        high = get_valid_input(f"Enter maximum value for '{name}': ", float)
        while low >= high:
            print("Minimum value must be less than maximum value.")
            low = get_valid_input(f"Enter minimum value for '{name}': ", float)
            high = get_valid_input(f"Enter maximum value for '{name}': ", float)
            
        dimensions.append(Real(low, high, name=name))

    return dimensions, param_names, []

def define_objective():
    """Asks the user to define the objective to optimize."""
    print("\n--- Define Your Optimization Objective ---")
    objective_name = get_valid_input(
        "What is the name of the value you want to optimize? (e.g., Yield, Purity, Conversion): ",
        str
    ).strip()

    optimization_type = get_valid_input(
        f"Do you want to maximize or minimize '{objective_name}'? (max/min): ",
        str,
        lambda x: x.lower() in ['max', 'min'],
        "Invalid choice. Please enter 'max' or 'min'."
    ).lower()

    return objective_name, optimization_type == 'max'

def get_initial_experiments(param_names, objective_name, dimensions):
    """Collects initial experimental data from the user, with bounds checking."""
    print("\n--- Enter Your Initial Experimental Data ---")
    num_experiments = get_valid_input(
        "How many experiments have you already performed? ",
        int,
        lambda x: x >= 0,
        "Please enter a non-negative integer."
    )

    if num_experiments == 0:
        print("Starting without initial experimental data. The first few suggestions will be exploratory.")
        print("Consider running at least 5-10 initial random experiments to help the model learn faster.")
        return [], []

    X_initial = []
    y_initial = []

    for i in range(num_experiments):
        print(f"\nExperiment {i + 1}:")
        current_x = []
        for j, param_name in enumerate(param_names):
            low = dimensions[j].low
            high = dimensions[j].high
            value = get_valid_input(
                f"   Enter value for '{param_name}' (between {low} and {high}): ",
                float,
                lambda v: low <= v <= high,
                f"Value must be between {low} and {high}."
            )
            current_x.append(value)

        X_initial.append(current_x)
        objective_value = get_valid_input(f"   Enter measured value for '{objective_name}': ", float)
        y_initial.append(objective_value)

    return X_initial, y_initial

def select_model_and_acquisition():
    """Allows the user to select the surrogate model and acquisition function."""
    print("\n--- Configure Bayesian Optimization Model ---")

    model_choices = {
        'gp': GaussianProcessRegressor(), 
        'rf': RandomForestRegressor(n_estimators=100), 
        'et': ExtraTreesRegressor(n_estimators=100)
    }
    
    model_name = get_valid_input(
        "Choose a surrogate model (GP - Gaussian Process, RF - Random Forest, ET - Extra Trees): ",
        str,
        lambda x: x.lower() in ['gp', 'rf', 'et'],
        "Invalid model choice. Please enter 'GP', 'RF', or 'ET'."
    ).lower()
    
    base_estimator = model_choices[model_name]
    
    if model_name == 'gp':
        print("Gaussian Process (GP) is generally good for smooth functions and provides uncertainty estimates.")
    else:
        print(f"{model_name.upper()} (Tree-based) models are good for non-linear, potentially non-smooth functions.")

    acq_func_choices = {
        'ei': 'EI',
        'pi': 'PI',
        'lcb': 'LCB',
        'gp_hedge': 'gp_hedge'
    }
    
    acq_func_name = get_valid_input(
        "Choose an acquisition function (EI - Expected Improvement, PI - Probability of Improvement, LCB - Lower Confidence Bound, gp_hedge): ",
        str,
        lambda x: x.lower() in acq_func_choices,
        "Invalid acquisition function choice. Please enter 'EI', 'PI', 'LCB', or 'gp_hedge'."
    ).lower()
    
    acq_func = acq_func_choices[acq_func_name]
    
    explanations = {
        'ei': "Expected Improvement (EI) balances exploring new regions and exploiting promising ones.",
        'pi': "Probability of Improvement (PI) is more exploitative, focusing near the current best.",
        'lcb': "Lower Confidence Bound (LCB) is more explorative, favoring uncertain regions.",
        'gp_hedge': "GP_Hedge dynamically switches between EI, PI, and LCB, often providing robust performance."
    }
    
    print(explanations[acq_func_name])

    return base_estimator, acq_func

def main():
    dimensions, param_names, _ = define_parameters()
    objective_name, maximize_objective = define_objective()
    
    if maximize_objective:
        print(f"Note: Since you want to maximize '{objective_name}', values will be internally negated for minimization.")
    else:
        print(f"Note: Since you want to minimize '{objective_name}', values will be used directly.")

    X_initial, y_initial = get_initial_experiments(param_names, objective_name, dimensions)
    
    if maximize_objective:
        y_initial = [-val for val in y_initial]

    base_estimator, acq_func = select_model_and_acquisition()

    # Improved initial points calculation
    initial_random_points = min(10, max(5, len(X_initial)))
    
    print("\nInitializing Bayesian Optimizer...")
    optimizer = Optimizer(
        dimensions=dimensions,
        base_estimator=base_estimator,
        acq_func=acq_func,
        n_initial_points=initial_random_points 
    )

    if X_initial:
        optimizer.tell(X_initial, y_initial)
        print(f"Loaded {len(X_initial)} initial experiments into the optimizer.")
    else:
        print(f"No initial experiments provided. Using {initial_random_points} random points for initial exploration.")

    iteration = 0
    while True:
        iteration += 1
        print(f"\n--- Optimization Iteration {iteration} ---")

        if optimizer.yi:
            best_index = np.argmin(optimizer.yi)
            current_best_y_internal = optimizer.yi[best_index]
            current_best_x = optimizer.Xi[best_index]

            # Revert negation for display
            current_best_y_display = -current_best_y_internal if maximize_objective else current_best_y_internal

            print(f"\nCurrent best {objective_name} found so far: {current_best_y_display:.4f}")
            print("Corresponding parameters:")
            for name, value in zip(param_names, current_best_x):
                print(f"   {name}: {format_param_value(value)}")
        else:
            print(f"No results yet. Performing initial exploration ({len(optimizer.yi)}/{initial_random_points} points)...")

        confirm_continue = get_valid_input(
            "\nReady to get the next suggestion? (yes/no): ",
            str,
            lambda x: x.lower() in ['yes', 'no'],
            "Please enter 'yes' or 'no'."
        ).lower()

        if confirm_continue == 'no':
            print("\nOptimization session ended. Goodbye!")
            break

        # Suggest next parameters
        suggested_x = optimizer.ask()
        X_transformed = optimizer.space.transform([suggested_x])
        
        # Enhanced model prediction with tree-based uncertainty
        if optimizer.models:
            model = optimizer.models[-1]
            
            if isinstance(model, GaussianProcessRegressor):
                mean, std = model.predict(X_transformed, return_std=True)
                expected_objective_internal = mean[0]
                uncertainty = std[0]
                
            elif isinstance(model, (RandomForestRegressor, ExtraTreesRegressor)):
                # Get predictions from all trees
                predictions = [tree.predict(X_transformed)[0] for tree in model.estimators_]
                expected_objective_internal = np.mean(predictions)
                uncertainty = np.std(predictions)
                
            else:
                mean = model.predict(X_transformed)
                expected_objective_internal = mean[0]
                uncertainty = "N/A"
        else:
            expected_objective_internal = "N/A"
            uncertainty = "N/A"

        # Revert negation for display
        if isinstance(expected_objective_internal, (float, int)):
            expected_display = -expected_objective_internal if maximize_objective else expected_objective_internal
        else:
            expected_display = expected_objective_internal

        print("\nSuggested next experiment parameters:")
        for name, value in zip(param_names, suggested_x):
            print(f"   {name}: {format_param_value(value)}")
            
        print(f"Expected {objective_name}: {format_param_value(expected_display)}")
        print(f"Uncertainty: {format_param_value(uncertainty) if isinstance(uncertainty, (int, float)) else uncertainty}")

        # Get actual result
        actual_result = get_valid_input(
            f"\nAfter performing the experiment, what was the actual {objective_name}? : ",
            float
        )
        
        if maximize_objective:
            internal_result = -actual_result
        else:
            internal_result = actual_result

        optimizer.tell(suggested_x, internal_result)
        print("Model updated with your new experimental result.")

if __name__ == "__main__":
    main()
